{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff494dac-e03e-4106-b994-5c4523e1735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Belinda\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# 1) Load Core Data & Model \n",
    "\n",
    "df = pd.read_csv('data_cleaned_LDA_final.csv')\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "print(\"spaCy model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ac7597-8b6d-427e-b8d4-d6cf204b6518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique O*NET + ESCO skills combined: 107890\n"
     ]
    }
   ],
   "source": [
    "# 2) Build skill list (dedupe + filter + whitelist)\n",
    "\n",
    "# LOAD O*NET FILES \n",
    "df_tech    = pd.read_csv(\"Technology_Skills.csv\", encoding=\"utf-8\")\n",
    "df_general = pd.read_csv(\"Skills.csv\",            encoding=\"utf-8\")\n",
    "\n",
    "tech_skills = df_tech[\"Example\"].dropna().astype(str).tolist()\n",
    "general_skills = df_general[\"Element Name\"].dropna().astype(str).tolist()\n",
    "\n",
    "# LOAD ESCO FILE AND PROCESS LABELS \n",
    "df_esco = pd.read_csv(\"skills_en.csv\", encoding=\"utf-8\", on_bad_lines='skip')\n",
    "\n",
    "# Get the preferred skill names\n",
    "esco_preferred = df_esco['preferredLabel'].dropna().astype(str).tolist()\n",
    "\n",
    "# Get the altLabels, stored as '\\n'-separated strings, splits those strings into individual synonyms and flattens the list.\n",
    "esco_alt_labels = df_esco['altLabels'].dropna().apply(\n",
    "    lambda x: x.split('\\n')\n",
    ").explode().astype(str).tolist()\n",
    "\n",
    "\n",
    "# --- COMBINE, DEDUPLICATE, AND CLEAN ALL SOURCES ---\n",
    "\n",
    "# Combine all lists (O*NET examples + O*NET general + ESCO preferred + ESCO alt labels)\n",
    "all_skills_raw = tech_skills + general_skills + esco_preferred + esco_alt_labels\n",
    "\n",
    "# Deduplicate and lowercase the entire combined dictionary\n",
    "skill_list = list(dict.fromkeys(s.strip().lower() for s in all_skills_raw if s and s.strip()))\n",
    "print(f\"Total unique O*NET + ESCO skills combined: {len(skill_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b670bbab-ae2f-4305-ae82-2da26ae69078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final O*NET + ESCO skill list size after filtering: 107860\n"
     ]
    }
   ],
   "source": [
    "# --- APPLY FILTERING LOGIC ---\n",
    "\n",
    "MIN_SKILL_LENGTH = 3\n",
    "SHORT_WHITELIST = {\"r\",\"go\",\"c\",\"c#\",\"c++\",\".net\"}\n",
    "generic_phrases_to_exclude = {\n",
    "    \"instagram\",\"facebook\",\"whatsapp\",\"email\",\"telephone\",\"call\",\n",
    "    \"telegram\",\"twitter\",\"youtube\",\"linkedin\",\"zoom\", \"writing\", \"monitoring\", \"speaking\"  \n",
    "}\n",
    "\n",
    "skill_list_filtered = [\n",
    "    s for s in skill_list\n",
    "    if ((len(s.split()) > 1) or (len(s) >= MIN_SKILL_LENGTH) or (s in SHORT_WHITELIST))\n",
    "    and (s not in generic_phrases_to_exclude)\n",
    "]\n",
    "print(f\"Final O*NET + ESCO skill list size after filtering: {len(skill_list_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71b01ff-7cb8-4ff1-82d7-d4586265ef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhraseMatcher initialized with 107860 patterns.\n",
      "\n",
      "Starting extraction on 65314 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Belinda\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O*NET_ESCO-based skill extraction is COMPLETE (Optimized).\n"
     ]
    }
   ],
   "source": [
    "# 3) PhraseMatcher (prefer longest)\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "for s in skill_list_filtered:\n",
    "    matcher.add(s, [nlp.make_doc(s)])\n",
    "\n",
    "print(f\"PhraseMatcher initialized with {len(matcher)} patterns.\")\n",
    "\n",
    "# 4) Batch extraction\n",
    "df = df.reset_index(drop=True)\n",
    "job_texts_full = df[\"clean_title+desc\"].fillna(\"\").astype(str).tolist()\n",
    "print(f\"\\nStarting extraction on {len(job_texts_full)} documents...\")\n",
    "\n",
    "extracted = []\n",
    "for doc in nlp.pipe(job_texts_full, batch_size=256):\n",
    "    # Get all matches, including overlapping ones\n",
    "    matches = matcher(doc) \n",
    "    \n",
    "    # Post-process to filter out shorter, overlapping matches \n",
    "    # Sort matches by length (e - s) descending to prioritize longer matches\n",
    "    matches_sorted = sorted(matches, key=lambda x: (x[2] - x[1]), reverse=True)\n",
    "    \n",
    "    seen_spans = [] \n",
    "    filtered_matches = set()\n",
    "    \n",
    "    for match_id, start, end in matches_sorted:\n",
    "        is_overlapping = False\n",
    "        # Check if the current match is contained within an already kept and therefore longer match\n",
    "        for seen_start, seen_end in seen_spans:\n",
    "            if start >= seen_start and end <= seen_end:\n",
    "                is_overlapping = True\n",
    "                break\n",
    "        \n",
    "        if not is_overlapping:\n",
    "            # Keep the longer match and mark its span as seen\n",
    "            filtered_matches.add(nlp.vocab.strings[match_id])\n",
    "            seen_spans.append((start, end))\n",
    "\n",
    "    # Append the sorted list of unique, non-overlapping skills\n",
    "    extracted.append(sorted(filtered_matches))\n",
    "\n",
    "df[\"extracted_skills_list\"] = extracted\n",
    "print(\"\\nO*NET_ESCO-based skill extraction is COMPLETE (Optimized).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00665eee-e417-4646-ab41-bd7d55206ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to 'data_with_extracted_skills_ONET_ESCO.csv'.\n",
      "\n",
      "--- Skill Matching Statistics (O*NET_ESCO) ---\n",
      "Total documents: 65314\n",
      "Documents with matched skills: 64495\n",
      "Percentage of matched documents: 98.75%\n"
     ]
    }
   ],
   "source": [
    "# 5) Overview of extraction\n",
    "out = \"data_with_extracted_skills_ONET_ESCO.csv\"\n",
    "df.to_csv(out, index=False)\n",
    "print(f\"Final dataset saved to '{out}'.\")\n",
    "\n",
    "total_rows = len(df)\n",
    "matched = sum(bool(x) for x in df[\"extracted_skills_list\"])\n",
    "print(f\"\\n--- Skill Matching Statistics (O*NET_ESCO) ---\")\n",
    "print(f\"Total documents: {total_rows}\")\n",
    "print(f\"Documents with matched skills: {matched}\")\n",
    "print(f\"Percentage of matched documents: {matched/total_rows*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92459daf-4493-44dd-9e10-f44dd6f424e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skill_count_distribution_combined.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "try:\n",
    "    df['extracted_skills_list'] = df['extracted_skills_list'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.strip().startswith('[') else x\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Warning: literal_eval skipped. Assuming lists already. Error: {e}\")\n",
    "\n",
    "# Build counts\n",
    "skill_counts = [len(x) for x in df['extracted_skills_list'] if isinstance(x, list)]\n",
    "\n",
    "if len(skill_counts) == 0:\n",
    "    print(\"No skills found to plot. Check the matcher/dictionary and input text.\")\n",
    "else:\n",
    "    # Basic stats\n",
    "    mean_count   = float(np.mean(skill_counts))\n",
    "    median_count = float(np.median(skill_counts))\n",
    "    stdev_count  = float(np.std(skill_counts))\n",
    "    p95_count    = float(np.quantile(skill_counts, 0.95))\n",
    "    p99_count    = float(np.quantile(skill_counts, 0.99))\n",
    "    nonzero_pct  = 100.0 * (np.sum(np.array(skill_counts) > 0) / len(skill_counts))\n",
    "\n",
    "    # Dynamic axis range (cap at 40 to keep it readable)\n",
    "    MAX_SKILLS = int(min(40, max(skill_counts) if skill_counts else 40))\n",
    "    BIN_COUNT  = MAX_SKILLS  # 1 bin per integer count\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6), dpi=100)\n",
    "    plt.hist(skill_counts, bins=BIN_COUNT, range=(0, MAX_SKILLS))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Annotation\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    xtext = MAX_SKILLS * 0.72\n",
    "    ax.text(xtext, ymax*0.92, f\"Mean   : {mean_count:.1f}\", fontsize=10)\n",
    "    ax.text(xtext, ymax*0.86, f\"Median : {median_count:.1f}\", fontsize=10)\n",
    "    ax.text(xtext, ymax*0.80, f\"Stdev  : {stdev_count:.1f}\", fontsize=10)\n",
    "    ax.text(xtext, ymax*0.74, f\"p95    : {p95_count:.1f}\", fontsize=10)\n",
    "    ax.text(xtext, ymax*0.68, f\"p99    : {p99_count:.1f}\", fontsize=10)\n",
    "    ax.text(xtext, ymax*0.62, f\"% â‰¥1   : {nonzero_pct:.1f}%\", fontsize=10)\n",
    "\n",
    "    # Labels/format\n",
    "    ax.set(xlim=(0, MAX_SKILLS),\n",
    "           ylabel='Number of Job Postings',\n",
    "           xlabel='Number of Extracted Skills per Job')\n",
    "    ax.set_xticks(np.arange(0, MAX_SKILLS+1, max(1, MAX_SKILLS//8)))\n",
    "    ax.set_title('Distribution of Extracted Skills per Job Posting (O*NET_ESCO)', fontsize=14)\n",
    "    ax.grid(axis='y', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    plt.savefig('skill_count_distribution_combined.png')\n",
    "    plt.close(fig)\n",
    "    print(\"skill_count_distribution_combined.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
