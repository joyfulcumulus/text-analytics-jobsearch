{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08fd5ace-b99c-4840-9771-85c32f077018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilbert_job_category_classifier.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c3dbd0-5034-45e5-acff-ae85655d7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "TEXT_COL = \"processed_text\"\n",
    "LABEL_COL = \"merged_category\"\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "SAVE_DIR = \"./distilbert_job_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7faa6a7e-3d58-4405-a6d0-b3b5d47cb601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job_id', 'job_title', 'company', 'descriptions', 'State', 'merged_category', 'category', 'subcategory', 'role_clean', 'type_clean', 'salary', 'processed_title+desc', 'processed_text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data_cleaned.csv\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60aa039-29be-409a-9490-516196e45ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 65878 job postings, 14 categories.\n",
      "Columns: ['job_id', 'job_title', 'company', 'descriptions', 'State', 'merged_category', 'category', 'subcategory', 'role_clean', 'type_clean', 'salary', 'processed_title+desc', 'processed_text']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------\n",
    "\n",
    "df = pd.read_csv(\"data_cleaned.csv\")\n",
    "\n",
    "# Explicitly set correct columns\n",
    "TEXT_COL = \"processed_text\"\n",
    "LABEL_COL = \"merged_category\"\n",
    "\n",
    "# Drop rows missing text or labels\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} job postings, {df[LABEL_COL].nunique()} categories.\")\n",
    "print(\"Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2598ae4-1ec7-44f2-b83f-1579f6d59857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split complete — Train: 46114, Val: 13176, Test: 6588\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Encode labels and split dataset\n",
    "# -------------------------\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[LABEL_COL])\n",
    "joblib.dump(le, os.path.join(SAVE_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "# Split 70% train / 20% val / 10% test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df[\"label\"], random_state=42)\n",
    "print(f\"✅ Split complete — Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae9a7c9-6849-4133-ab24-2933b9ec2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Dataset class & tokenizer\n",
    "# -------------------------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class JobDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_ds = JobDataset(train_df[TEXT_COL].tolist(), train_df[\"label\"].tolist())\n",
    "val_ds = JobDataset(val_df[TEXT_COL].tolist(), val_df[\"label\"].tolist())\n",
    "test_ds = JobDataset(test_df[TEXT_COL].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce381334-4547-4041-8a15-30a6c0c3c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 4. Model setup\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = df[\"label\"].nunique()\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55a0bc9-559b-4757-b5aa-4867d4904452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5. Training & evaluation functions\n",
    "# -------------------------\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return total_loss / len(loader), preds, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be10cbe0-a037-4c6c-b50a-3d9203407796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54183656-9645-4751-af37-b41a539d1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6. Training loop \n",
    "# -------------------------\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
    "    \n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # --- Metrics ---\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"Validation classification report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=le.classes_))\n",
    "    \n",
    "    # --- Save best model ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained(SAVE_DIR)\n",
    "        tokenizer.save_pretrained(SAVE_DIR)\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f9499-bc6a-449d-9ab6-8c56c089cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7. Test evaluation\n",
    "# -------------------------\n",
    "print(\"\\nLoading best model for testing...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(SAVE_DIR).to(device)\n",
    "test_loss, test_preds, test_true = eval_epoch(model, test_loader)\n",
    "\n",
    "print(f\"\\nTest loss: {test_loss:.4f}\")\n",
    "print(classification_report(test_true, test_preds, target_names=le.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_true, test_preds)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix - DistilBERT Job Category Classification\")\n",
    "plt.xlabel(\"Predicted Category\")\n",
    "plt.ylabel(\"True Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"confusion_matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Training complete. Model saved to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa37de3-ab9d-4708-98a5-8bfee735735c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e72a2-4bee-47a4-bfc9-95b3fac24f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8307780e-ce52-417a-9968-85fd1ffe1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "TEXT_COL = \"processed_text\"\n",
    "LABEL_COL = \"merged_category\"\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LR = 2e-5\n",
    "SAVE_DIR = \"./distilbert_job_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f177186-7258-4b6c-9ff2-c65e198abe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Initial learning rate: 0.0\n",
      "\n",
      "===== Epoch 1/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000009\n",
      "Step 0/2883, Loss: 0.2213\n",
      "Step 100/2883, Loss: 0.0024\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 200/2883, Loss: 0.0029\n",
      "Step 300/2883, Loss: 0.0214\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000011\n",
      "Step 400/2883, Loss: 0.0502\n",
      "Step 500/2883, Loss: 0.2860\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000005\n",
      "Step 600/2883, Loss: 0.0117\n",
      "Step 700/2883, Loss: 0.4676\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 800/2883, Loss: 0.0029\n",
      "Step 900/2883, Loss: 0.0108\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1000/2883, Loss: 0.0050\n",
      "Step 1100/2883, Loss: 0.0033\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 1200/2883, Loss: 0.0107\n",
      "Step 1300/2883, Loss: 0.0028\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1400/2883, Loss: 0.0015\n",
      "Step 1500/2883, Loss: 0.0025\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 1600/2883, Loss: 0.0096\n",
      "Step 1700/2883, Loss: 0.2350\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000017\n",
      "Step 1800/2883, Loss: 0.4951\n",
      "Step 1900/2883, Loss: 0.0605\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000016\n",
      "Step 2000/2883, Loss: 0.1069\n",
      "Step 2100/2883, Loss: 0.0137\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000008\n",
      "Step 2200/2883, Loss: 0.0275\n",
      "Step 2300/2883, Loss: 0.1955\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2400/2883, Loss: 0.0029\n",
      "Step 2500/2883, Loss: 0.0055\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000041\n",
      "Step 2600/2883, Loss: 0.5170\n",
      "Step 2700/2883, Loss: 0.0089\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000011\n",
      "Step 2800/2883, Loss: 0.0342\n",
      "Average training loss: 0.1010\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "✅ Saved best model at epoch 1\n",
      "\n",
      "===== Epoch 2/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 0/2883, Loss: 0.0033\n",
      "Step 100/2883, Loss: 0.0154\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 200/2883, Loss: 0.0022\n",
      "Step 300/2883, Loss: 0.2502\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000032\n",
      "Step 400/2883, Loss: 0.1896\n",
      "Step 500/2883, Loss: 0.0030\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000028\n",
      "Step 600/2883, Loss: 0.2084\n",
      "Step 700/2883, Loss: 0.0033\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000011\n",
      "Step 800/2883, Loss: 0.3953\n",
      "Step 900/2883, Loss: 0.0367\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000011\n",
      "Step 1000/2883, Loss: 0.0911\n",
      "Step 1100/2883, Loss: 0.0046\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1200/2883, Loss: 0.0048\n",
      "Step 1300/2883, Loss: 0.3174\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1400/2883, Loss: 0.0012\n",
      "Step 1500/2883, Loss: 0.4687\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1600/2883, Loss: 0.0041\n",
      "Step 1700/2883, Loss: 0.0034\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 1800/2883, Loss: 0.0122\n",
      "Step 1900/2883, Loss: 0.0020\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 2000/2883, Loss: 0.0034\n",
      "Step 2100/2883, Loss: 0.3091\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000018\n",
      "Step 2200/2883, Loss: 0.0644\n",
      "Step 2300/2883, Loss: 0.0019\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2400/2883, Loss: 0.0041\n",
      "Step 2500/2883, Loss: 0.0162\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000023\n",
      "Step 2600/2883, Loss: 0.1692\n",
      "Step 2700/2883, Loss: 0.0042\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000035\n",
      "Step 2800/2883, Loss: 0.1243\n",
      "Average training loss: 0.1007\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 3/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 0/2883, Loss: 0.0026\n",
      "Step 100/2883, Loss: 0.0125\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000019\n",
      "Step 200/2883, Loss: 0.1528\n",
      "Step 300/2883, Loss: 0.0025\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 400/2883, Loss: 0.0041\n",
      "Step 500/2883, Loss: 0.0354\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 600/2883, Loss: 0.0339\n",
      "Step 700/2883, Loss: 0.2417\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 800/2883, Loss: 0.2217\n",
      "Step 900/2883, Loss: 0.3425\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1000/2883, Loss: 0.0155\n",
      "Step 1100/2883, Loss: 0.2878\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000040\n",
      "Step 1200/2883, Loss: 0.1450\n",
      "Step 1300/2883, Loss: 0.0042\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1400/2883, Loss: 0.0078\n",
      "Step 1500/2883, Loss: 0.0948\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000027\n",
      "Step 1600/2883, Loss: 0.0832\n",
      "Step 1700/2883, Loss: 0.0044\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1800/2883, Loss: 0.0050\n",
      "Step 1900/2883, Loss: 0.0030\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 2000/2883, Loss: 0.0145\n",
      "Step 2100/2883, Loss: 0.3297\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2200/2883, Loss: 0.0051\n",
      "Step 2300/2883, Loss: 0.2312\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2400/2883, Loss: 0.0047\n",
      "Step 2500/2883, Loss: 0.0016\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000049\n",
      "Step 2600/2883, Loss: 0.4338\n",
      "Step 2700/2883, Loss: 0.0018\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000018\n",
      "Step 2800/2883, Loss: 0.1820\n",
      "Average training loss: 0.1002\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 4/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 0/2883, Loss: 0.0220\n",
      "Step 100/2883, Loss: 0.0139\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000018\n",
      "Step 200/2883, Loss: 0.0604\n",
      "Step 300/2883, Loss: 0.0173\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000016\n",
      "Step 400/2883, Loss: 0.2050\n",
      "Step 500/2883, Loss: 0.1754\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 600/2883, Loss: 0.0038\n",
      "Step 700/2883, Loss: 0.0018\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000036\n",
      "Step 800/2883, Loss: 0.3687\n",
      "Step 900/2883, Loss: 0.2163\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000038\n",
      "Step 1000/2883, Loss: 0.4868\n",
      "Step 1100/2883, Loss: 0.0618\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000009\n",
      "Step 1200/2883, Loss: 0.2985\n",
      "Step 1300/2883, Loss: 0.4223\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1400/2883, Loss: 0.0022\n",
      "Step 1500/2883, Loss: 0.0071\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000018\n",
      "Step 1600/2883, Loss: 0.1523\n",
      "Step 1700/2883, Loss: 0.0040\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000006\n",
      "Step 1800/2883, Loss: 0.0275\n",
      "Step 1900/2883, Loss: 0.2257\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 2000/2883, Loss: 0.0019\n",
      "Step 2100/2883, Loss: 0.2183\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000020\n",
      "Step 2200/2883, Loss: 0.2595\n",
      "Step 2300/2883, Loss: 0.4075\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000010\n",
      "Step 2400/2883, Loss: 0.2659\n",
      "Step 2500/2883, Loss: 0.0200\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000027\n",
      "Step 2600/2883, Loss: 0.1605\n",
      "Step 2700/2883, Loss: 0.1267\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000006\n",
      "Step 2800/2883, Loss: 0.0243\n",
      "Average training loss: 0.1006\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 5/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000012\n",
      "Step 0/2883, Loss: 0.1996\n",
      "Step 100/2883, Loss: 0.0030\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000024\n",
      "Step 200/2883, Loss: 0.3627\n",
      "Step 300/2883, Loss: 0.0020\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000025\n",
      "Step 400/2883, Loss: 0.2875\n",
      "Step 500/2883, Loss: 0.0343\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000005\n",
      "Step 600/2883, Loss: 0.0100\n",
      "Step 700/2883, Loss: 0.0067\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 800/2883, Loss: 0.0028\n",
      "Step 900/2883, Loss: 0.0316\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1000/2883, Loss: 0.0037\n",
      "Step 1100/2883, Loss: 0.1686\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000019\n",
      "Step 1200/2883, Loss: 0.3582\n",
      "Step 1300/2883, Loss: 0.3522\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1400/2883, Loss: 0.0022\n",
      "Step 1500/2883, Loss: 0.0016\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1600/2883, Loss: 0.0038\n",
      "Step 1700/2883, Loss: 0.0020\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 1800/2883, Loss: 0.0017\n",
      "Step 1900/2883, Loss: 0.0044\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 2000/2883, Loss: 0.0013\n",
      "Step 2100/2883, Loss: 0.0020\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 2200/2883, Loss: 0.0035\n",
      "Step 2300/2883, Loss: 0.0101\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000008\n",
      "Step 2400/2883, Loss: 0.4127\n",
      "Step 2500/2883, Loss: 0.3310\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2600/2883, Loss: 0.0039\n",
      "Step 2700/2883, Loss: 0.0018\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2800/2883, Loss: 0.0007\n",
      "Average training loss: 0.1013\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 6/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 0/2883, Loss: 0.0071\n",
      "Step 100/2883, Loss: 0.0659\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 200/2883, Loss: 0.0054\n",
      "Step 300/2883, Loss: 0.0263\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 400/2883, Loss: 0.0022\n",
      "Step 500/2883, Loss: 0.0204\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000020\n",
      "Step 600/2883, Loss: 0.4788\n",
      "Step 700/2883, Loss: 0.0039\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000017\n",
      "Step 800/2883, Loss: 0.3178\n",
      "Step 900/2883, Loss: 0.0523\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1000/2883, Loss: 0.0033\n",
      "Step 1100/2883, Loss: 0.0033\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000005\n",
      "Step 1200/2883, Loss: 0.4908\n",
      "Step 1300/2883, Loss: 0.0035\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000010\n",
      "Step 1400/2883, Loss: 0.1600\n",
      "Step 1500/2883, Loss: 0.0076\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 1600/2883, Loss: 0.0011\n",
      "Step 1700/2883, Loss: 0.0064\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 1800/2883, Loss: 0.0187\n",
      "Step 1900/2883, Loss: 0.0016\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000014\n",
      "Step 2000/2883, Loss: 0.1568\n",
      "Step 2100/2883, Loss: 0.0056\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 2200/2883, Loss: 0.0031\n",
      "Step 2300/2883, Loss: 0.0127\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2400/2883, Loss: 0.0071\n",
      "Step 2500/2883, Loss: 0.0630\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 2600/2883, Loss: 0.0056\n",
      "Step 2700/2883, Loss: 0.0011\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000005\n",
      "Step 2800/2883, Loss: 0.0128\n",
      "Average training loss: 0.1014\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 7/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 0/2883, Loss: 0.0233\n",
      "Step 100/2883, Loss: 0.0105\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000016\n",
      "Step 200/2883, Loss: 0.1719\n",
      "Step 300/2883, Loss: 0.3397\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000007\n",
      "Step 400/2883, Loss: 0.0220\n",
      "Step 500/2883, Loss: 0.0015\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000031\n",
      "Step 600/2883, Loss: 0.3781\n",
      "Step 700/2883, Loss: 0.0348\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000027\n",
      "Step 800/2883, Loss: 0.0932\n",
      "Step 900/2883, Loss: 0.0036\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1000/2883, Loss: 0.0139\n",
      "Step 1100/2883, Loss: 0.1931\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000010\n",
      "Step 1200/2883, Loss: 0.0280\n",
      "Step 1300/2883, Loss: 0.0558\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000022\n",
      "Step 1400/2883, Loss: 0.5253\n",
      "Step 1500/2883, Loss: 0.0094\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 1600/2883, Loss: 0.0078\n",
      "Step 1700/2883, Loss: 0.2401\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 1800/2883, Loss: 0.0061\n",
      "Step 1900/2883, Loss: 0.0070\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000006\n",
      "Step 2000/2883, Loss: 0.0243\n",
      "Step 2100/2883, Loss: 0.0109\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2200/2883, Loss: 0.0043\n",
      "Step 2300/2883, Loss: 0.5515\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000036\n",
      "Step 2400/2883, Loss: 0.4956\n",
      "Step 2500/2883, Loss: 0.0356\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 2600/2883, Loss: 0.0029\n",
      "Step 2700/2883, Loss: 0.0032\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2800/2883, Loss: 0.0059\n",
      "Average training loss: 0.1007\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 8/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000011\n",
      "Step 0/2883, Loss: 0.0304\n",
      "Step 100/2883, Loss: 0.0024\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000029\n",
      "Step 200/2883, Loss: 0.2241\n",
      "Step 300/2883, Loss: 0.0275\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 400/2883, Loss: 0.0042\n",
      "Step 500/2883, Loss: 0.5102\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000017\n",
      "Step 600/2883, Loss: 0.1333\n",
      "Step 700/2883, Loss: 0.0082\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000016\n",
      "Step 800/2883, Loss: 0.1874\n",
      "Step 900/2883, Loss: 0.4597\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000009\n",
      "Step 1000/2883, Loss: 0.0367\n",
      "Step 1100/2883, Loss: 0.0520\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000027\n",
      "Step 1200/2883, Loss: 0.2616\n",
      "Step 1300/2883, Loss: 0.4959\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000008\n",
      "Step 1400/2883, Loss: 0.0445\n",
      "Step 1500/2883, Loss: 0.0051\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1600/2883, Loss: 0.0025\n",
      "Step 1700/2883, Loss: 0.0469\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 1800/2883, Loss: 0.0013\n",
      "Step 1900/2883, Loss: 0.0677\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2000/2883, Loss: 0.0015\n",
      "Step 2100/2883, Loss: 0.0150\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 2200/2883, Loss: 0.0029\n",
      "Step 2300/2883, Loss: 0.0038\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2400/2883, Loss: 0.0015\n",
      "Step 2500/2883, Loss: 0.0027\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2600/2883, Loss: 0.0130\n",
      "Step 2700/2883, Loss: 0.0540\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 2800/2883, Loss: 0.0130\n",
      "Average training loss: 0.1006\n",
      "Validation loss: 0.2042\n",
      "Validation classification report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "      Accounting & Finance       0.97      0.98      0.98      2633\n",
      "            Administration       0.96      0.94      0.95      1367\n",
      "     Construction & Trades       0.92      0.91      0.91       567\n",
      "         Creative & Design       0.91      0.93      0.92       247\n",
      "                 Education       0.94      0.93      0.94       155\n",
      "  Engineering & Technology       0.96      0.96      0.96      3512\n",
      "                Healthcare       0.95      0.93      0.94       174\n",
      "    Hospitality & Services       0.92      0.94      0.93       803\n",
      "           Human Resources       0.95      0.97      0.96       775\n",
      "        Legal & Compliance       0.91      0.89      0.90        72\n",
      "     Management & Strategy       0.78      0.77      0.78        83\n",
      " Manufacturing & Logistics       0.93      0.95      0.94      1037\n",
      "         Sales & Marketing       0.95      0.94      0.95      1661\n",
      "Social & Community / Other       0.87      0.80      0.83        90\n",
      "\n",
      "                  accuracy                           0.95     13176\n",
      "                 macro avg       0.92      0.92      0.92     13176\n",
      "              weighted avg       0.95      0.95      0.95     13176\n",
      "\n",
      "\n",
      "===== Epoch 9/10 =====\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 0/2883, Loss: 0.0126\n",
      "Step 100/2883, Loss: 0.2794\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000005\n",
      "Step 200/2883, Loss: 0.0078\n",
      "Step 300/2883, Loss: 0.0229\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 400/2883, Loss: 0.0020\n",
      "Step 500/2883, Loss: 0.2014\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000003\n",
      "Step 600/2883, Loss: 0.0020\n",
      "Step 700/2883, Loss: 0.0044\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000004\n",
      "Step 800/2883, Loss: 0.0128\n",
      "Step 900/2883, Loss: 0.0049\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000014\n",
      "Step 1000/2883, Loss: 0.2031\n",
      "Step 1100/2883, Loss: 0.0060\n",
      "Grad check [distilbert.embeddings.word_embeddings.weight] mean=0.000002\n",
      "Step 1200/2883, Loss: 0.0013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# ✅ Gradient check (once per 200 steps)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6. Training loop (fixed version)\n",
    "# -------------------------\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# --- Sanity checks ---\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "for g in optimizer.param_groups:\n",
    "    print(\"Initial learning rate:\", g['lr'])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
    "    \n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to correct device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward + backward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # ✅ Gradient check (once per 200 steps)\n",
    "        if step % 200 == 0:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"Grad check [{name}] mean={param.grad.abs().mean().item():.6f}\")\n",
    "                    break\n",
    "\n",
    "        # Gradient clipping + optimization\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # ✅ must come after optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "    print(\"Validation classification report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=le.classes_))\n",
    "    \n",
    "    # --- Save best model ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained(SAVE_DIR)\n",
    "        tokenizer.save_pretrained(SAVE_DIR)\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c0340-4d23-4ccd-98dd-19ac760c2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7. Test evaluation\n",
    "# -------------------------\n",
    "print(\"\\nLoading best model for testing...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(SAVE_DIR).to(device)\n",
    "test_loss, test_preds, test_true = eval_epoch(model, test_loader)\n",
    "\n",
    "print(f\"\\nTest loss: {test_loss:.4f}\")\n",
    "print(classification_report(test_true, test_preds, target_names=le.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_true, test_preds)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix - DistilBERT Job Category Classification\")\n",
    "plt.xlabel(\"Predicted Category\")\n",
    "plt.ylabel(\"True Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"confusion_matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Training complete. Model saved to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd397c20-fd09-458c-b766-74b533e3d0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
